{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50a2c95-610f-480f-9c17-4acfd2669c8d",
   "metadata": {},
   "source": [
    "# <center>An Analysis of Transformative Works Created by Fans of the Harry Potter Series in Reaction to the Author's Public Political Comments</center>\n",
    "\n",
    "## <center>Project completed by Kymberlee McMaster on May 16th, 2022</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb7072-0ffe-4d1a-ad3c-08f3333a4b91",
   "metadata": {},
   "source": [
    "### <center>Introduction</center>\n",
    "\n",
    "Fans are often known for using their talents and dedication to create new media based on the things they enjoy. One of the best examples of this is the writing and reading of fanfiction, the practice in which amateur authors may take aspects from the original content that they enjoyed and transforming them into original works of their own creation. There are various methods that these authors use to share their works with other individuals who also enjoyed the original piece of media but one of the most common is to post the work to a dedicated site for the posting and reading of fanfiction. While there are quite a few options available, we'll be focusing on Archive of Our Own, known colloquially as AO3, for our purposes as AO3's built in tagging and data storage system will allow us to search through the works of fiction using the author's own tags for their work rather than attempting to create tags ourself. \n",
    "\n",
    "However, since AO3 currently has over nine million works, in order to better analyze the data associated with the site and trends of fanfiction authors, we'll be focusing on writings by fans of a specific piece of media: the Harry Potter series written by J.K. Rowling.[[1]](https://archiveofourown.org/works/search?work_search%5Bquery%5D=) Additionally, we'll be specifically be focusing on the fanfiction written around a specific date in time as there are over 300,000 works for that series alone. \n",
    "\n",
    "On June 6th of 2020, author J.K. Rowling took to Twitter to express her displeasure over the use of the phrase \"people who menstruate\" rather than the word women.[[2]](https://www.glamour.com/story/a-complete-breakdown-of-the-jk-rowling-transgender-comments-controversy) This tweet and the subsequent tweets that followed it came under a lot of backlash with trans activists and fans of the Harry Potter series. This was not the first time that author J.K. Rowling had expressed such views and received backlash, but it is one of the most notable, so we will be aanalyzing works of fanfiction posted onto AO3 for the two weeks before the tweet was made and the two weeks following the tweet to view the potential impact that Rowling's postings may have had on the writings of the LGBTQIA+ community members and their allies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b77fe-964f-44f7-8be8-e39bc29909aa",
   "metadata": {},
   "source": [
    "### <center>Data Collection</center>\n",
    "\n",
    "As AO3 does not have a built in API, we will need to build our own method of scraping the data found on the site. In order to collect the data and avoid unneccesary scraping we'll be using AO3's built in search function to pre-search for works that were created between our dates of interest: May 23rd,2020 and June 19th, 2020. We do this by accessing the Works Search page located [here](https://archiveofourown.org/works/search), and entering our parameters into the Any Search field: created_at:[\"2020-05-23\" TO \"2020-06-19\"]. As well as selecting the English language option. This will generate the link that we can use in the data scraper that will gather the information about the works for us, located [here](https://archiveofourown.org/works/search?commit=Search&page=1&work_search[bookmarks_count]=&work_search[character_names]=&work_search[comments_count]=&work_search[complete]=&work_search[creators]=&work_search[crossover]=&work_search[fandom_names]=Harry+Potter+-+J.+K.+Rowling&work_search[freeform_names]=&work_search[hits]=&work_search[kudos_count]=&work_search[language_id]=en&work_search[query]=created_at%3A[%222020-05-23%22+TO+%222020-06-19%22]&work_search[rating_ids]=&work_search[relationship_names]=&work_search[revised_at]=&work_search[single_chapter]=0&work_search[sort_column]=created_at&work_search[sort_direction]=asc&work_search[title]=&work_search[word_count]=). \n",
    "\n",
    "By looking at our search results, we will see that we will be scraping the information about 3,523 works that were created and area available publicly without an account in the month time period we've identified. Below, we will first import the libararies necessary for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ef4c48-3b62-48a0-b4bc-0de6f8efaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries necessary to complete this project \n",
    "import requests\n",
    "import math\n",
    "from bs4 import BeautifulSoup \n",
    "import csv\n",
    "import re \n",
    "import random\n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import datetime \n",
    "\n",
    "\n",
    "import json \n",
    "import os.path \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3800a9-3871-4a4f-be45-ae364192eebf",
   "metadata": {},
   "source": [
    "Next, we need to inspect the page to see how the data on each page is stored. By using the developers tools, we can see that the results of the search are displayed in a class identified as a “works index group\" and each work is a list item below that with the role \"article\". We know that there are 3,523 works to be consumed, and there are 20 works displayed on each search page so we'll need to request the informtion from 177 pages. We split the URL into parts before and after the page number is stored to that we can complete our requests through an iterative process which automatically updates the page number used to request data. Then we initialize a couple new files to store the content from the scraping as we'll be completing it in page portions and that would make it difficult to store as a pandas dataframe right off the bat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b645645a-3f04-4eb3-9936-42e3ac15bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the URL into parts and store the current page number with those parts \n",
    "urlpt1 = \"https://archiveofourown.org/works/search?commit=Search&page=\"\n",
    "currpagenum = 1\n",
    "urlpt2 = \"&work_search[bookmarks_count]=&work_search[character_names]=&work_search[comments_count]=&work_search[complete]=&work_search[creators]=&work_search[crossover]=&work_search[fandom_names]=Harry+Potter+-+J.+K.+Rowling&work_search[freeform_names]=&work_search[hits]=&work_search[kudos_count]=&work_search[language_id]=en&work_search[query]=created_at%3A[%222020-05-23%22+TO+%222020-06-19%22]&work_search[rating_ids]=&work_search[relationship_names]=&work_search[revised_at]=&work_search[single_chapter]=0&work_search[sort_column]=created_at&work_search[sort_direction]=asc&work_search[title]=&work_search[word_count]=\"\n",
    "\n",
    "#Identified the number of works and pages that the scraper will need to iterate through\n",
    "works = 3523\n",
    "pages = math.ceil(works/20)\n",
    "\n",
    "#Iniate a new file to store the basic content from the scraping \n",
    "header = ['Title', 'Author', 'ID', 'Date_updated', 'Rating', 'Pairing', 'Warning', 'Complete', 'Language', 'Word_count', 'Num_chapters', 'Num_comments', 'Num_kudos', 'Num_bookmarks', 'Num_hits', 'Tags', 'Summary']\n",
    "with open('storedbasic.csv','w', encoding='utf8') as storedbasic:\n",
    "    writer = csv.writer(storedbasic)\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b8e33-ef19-4dd5-8b3a-b256603d9318",
   "metadata": {},
   "source": [
    "Now that we've completed some of our basic work, we can begin to design some of the functions we'll need to call to scrape the data out of the page. Specifically, we'll need one function to read in the content off the page. One piece of information that we could scrape but are choosing not to for the purpose of this project is the actual Comments on the published works. While this could have some interested information for us to take a look at, we are already dealing with an extremely large amount of data and the Comments section of each work does not have any bearing on what our true aim is with this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d49c95-6902-4238-bdf4-ddc7f784cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to gather all data \n",
    "def basicdata(mysoup): \n",
    "    #Initialize a set of variables to store all titles and info for page to add to the CSV all at once \n",
    "    titles = []\n",
    "    authors = []\n",
    "    ids = []\n",
    "    date_updated = []\n",
    "    ratings = []\n",
    "    pairings = []\n",
    "    warnings = []\n",
    "    complete = []\n",
    "    languages = []\n",
    "    word_count = []\n",
    "    chapters = []\n",
    "    comments = []\n",
    "    kudos = []\n",
    "    bookmarks = []\n",
    "    hits = []\n",
    "    tags = []\n",
    "    summary = []\n",
    "    \n",
    "    for article in mysoup.find_all('li', {'role':'article'}):\n",
    "        titles.append(article.find('h4', {'class':'heading'}).find('a').text)\n",
    "        try:\n",
    "            authors.append(article.find('a', {'rel':'author'}).text)\n",
    "        except:\n",
    "            authors.append('Anonymous')\n",
    "        ids.append(article.find('h4', {'class':'heading'}).find('a').get('href')[7:])\n",
    "        date_updated.append(article.find('p', {'class':'datetime'}).text)\n",
    "        ratings.append(article.find('span', {'class':re.compile(r'rating\\-.*rating')}).text)\n",
    "        pairings.append(article.find('span', {'class':re.compile(r'category\\-.*category')}).text)\n",
    "        warnings.append(article.find('span', {'class':re.compile(r'warning\\-.*warnings')}).text)\n",
    "        complete.append(article.find('span', {'class':re.compile(r'complete\\-.*iswip')}).text)\n",
    "        languages.append(article.find('dd', {'class':'language'}).text)\n",
    "        tags.append(article.find('ul', {'class':'tags commas'}).text)\n",
    "        count = article.find('dd', {'class':'words'}).text\n",
    "        if len(count) > 0:\n",
    "            word_count.append(count)\n",
    "        else:\n",
    "            word_count.append('0')\n",
    "        chapters.append(article.find('dd', {'class':'chapters'}).text.split('/')[0])\n",
    "        try:\n",
    "            comments.append(article.find('dd', {'class':'comments'}).text)\n",
    "        except:\n",
    "            comments.append('0')\n",
    "        try:\n",
    "            kudos.append(article.find('dd', {'class':'kudos'}).text)\n",
    "        except:\n",
    "            kudos.append('0')\n",
    "        try:\n",
    "            bookmarks.append(article.find('dd', {'class':'bookmarks'}).text)\n",
    "        except:\n",
    "            bookmarks.append('0')\n",
    "        try:\n",
    "            hits.append(article.find('dd', {'class':'hits'}).text)\n",
    "        except:\n",
    "            hits.append('0')\n",
    "        #try: \n",
    "            #tags.append(article.find('span', {'class':re.compile(r'freeforms\\-.*freeforms')}).text)\n",
    "        #except: \n",
    "            #tags.append(' ')\n",
    "        try:\n",
    "            summary.append(article.find('blockquote', {'class':'userstuff summary'}).text)\n",
    "        except: \n",
    "            summary.append(' ')\n",
    "            \n",
    "            \n",
    "    df = pd.DataFrame(list(zip(titles, authors, ids, date_updated, ratings, pairings,\\\n",
    "                              warnings, complete, languages, word_count, chapters,\\\n",
    "                               comments, kudos, bookmarks, hits, tags, summary)))\n",
    "    \n",
    "    with open('storedbasic.csv','a', encoding='utf8') as storedbasic:\n",
    "        df.to_csv(storedbasic, header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870da39-bc14-4c6c-991d-8f84693fe196",
   "metadata": {},
   "source": [
    "With our helper function for our basic data, we can now  iterate through the pages of the searched works and gather the basic data into the CSV files previously created. Due to AO3's built in site protections, We will scrape by increments of 100 pages and pause between each code block execution in order to ensure that we can gather all of the data we are trying to request from the site.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1a862a-4514-4178-8b0d-a39f8b45c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a break from parsing. Current page count is: 100\n",
      "Will resume in 10 min\n",
      "Wait time is over, will resume parsing now.\n",
      "Parsing has finished, the remainder of basic data has been consumed\n"
     ]
    }
   ],
   "source": [
    "#Reset page number in case anything has gotten messed up with the block\n",
    "currpagenum = 1\n",
    "\n",
    "#Set the page by using the page number, and the URL parts\n",
    "page = requests.get(urlpt1 + str(currpagenum) + urlpt2)\n",
    "\n",
    "#Use BeautifulSoup to parse the data as html\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "#This for loop will iterate through the pages and add the basic data to the basic data table \n",
    "for i in range(1, pages + 1): \n",
    "    \n",
    "    url = urlpt1 + str(currpagenum) + urlpt2 \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    basicdata(soup) \n",
    "    \n",
    "    currpagenum += 1\n",
    "    \n",
    "    if (i % 100) == 0 : \n",
    "        print(\"Taking a break from parsing. Current page count is: \" + str(currpagenum - 1) )\n",
    "        print(\"Will resume in 10 min\")\n",
    "        time.sleep(600)\n",
    "        print(\"Wait time is over, will resume parsing now.\")\n",
    "    \n",
    "print(\"Parsing has finished, the remainder of basic data has been consumed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ead2c-5f42-4736-b382-85c17e531ed3",
   "metadata": {},
   "source": [
    "The above print statements are simply to show that the parser has completed running and is moving on to the next code snippet where we will store the information we've collected in a pandas dataframe and verify that our parsing was successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872344c6-4c1e-4ddb-80b5-a399d07de9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date_updated</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Pairing</th>\n",
       "      <th>Warning</th>\n",
       "      <th>Complete</th>\n",
       "      <th>Language</th>\n",
       "      <th>Word_count</th>\n",
       "      <th>Num_chapters</th>\n",
       "      <th>Num_comments</th>\n",
       "      <th>Num_kudos</th>\n",
       "      <th>Num_bookmarks</th>\n",
       "      <th>Num_hits</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This Love I Have Inside</td>\n",
       "      <td>stargazing_dreamer_girl</td>\n",
       "      <td>24329029</td>\n",
       "      <td>23 May 2020</td>\n",
       "      <td>General Audiences</td>\n",
       "      <td>F/M</td>\n",
       "      <td>No Archive Warnings Apply</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>9,403</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>612</td>\n",
       "      <td>\\nNo Archive Warnings ApplyFred Weasley/Origin...</td>\n",
       "      <td>\\nClara Comder, student at Hogwarts School of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spelling It Out</td>\n",
       "      <td>Nocturnal_Daydreams</td>\n",
       "      <td>24329044</td>\n",
       "      <td>23 May 2020</td>\n",
       "      <td>Not Rated</td>\n",
       "      <td>F/M, Other</td>\n",
       "      <td>Choose Not To Use Archive Warnings</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>29,407</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>213</td>\n",
       "      <td>32</td>\n",
       "      <td>5338</td>\n",
       "      <td>\\nCreator Chose Not To Use Archive WarningsHer...</td>\n",
       "      <td>\\nTattoos or Birthmarks of your soulmates firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Untamed Journey</td>\n",
       "      <td>Jetainia</td>\n",
       "      <td>24329050</td>\n",
       "      <td>23 May 2020</td>\n",
       "      <td>General Audiences</td>\n",
       "      <td>F/M</td>\n",
       "      <td>No Archive Warnings Apply</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>1,612</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>\\nNo Archive Warnings ApplyHelga Hufflepuff/Sa...</td>\n",
       "      <td>\\nThe roaming Hogwarts saloon is a place of ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Quill</td>\n",
       "      <td>xslytherclawx</td>\n",
       "      <td>24329686</td>\n",
       "      <td>23 May 2020</td>\n",
       "      <td>Teen And Up Audiences</td>\n",
       "      <td>M/M</td>\n",
       "      <td>No Archive Warnings Apply</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>586</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>664</td>\n",
       "      <td>\\nNo Archive Warnings ApplyDraco Malfoy/Harry ...</td>\n",
       "      <td>\\nHarry's always wondered about Draco's golden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Potter Bending Force</td>\n",
       "      <td>Gman85</td>\n",
       "      <td>24329902</td>\n",
       "      <td>19 Jan 2022</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>F/M</td>\n",
       "      <td>Underage</td>\n",
       "      <td>Work in Progress</td>\n",
       "      <td>English</td>\n",
       "      <td>128,631</td>\n",
       "      <td>17</td>\n",
       "      <td>104</td>\n",
       "      <td>286</td>\n",
       "      <td>133</td>\n",
       "      <td>21617</td>\n",
       "      <td>\\nUnderageOther Relationship Tags to Be Added ...</td>\n",
       "      <td>\\nHarry was woefully unprepared for the Tri-Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>Ante Astra</td>\n",
       "      <td>Evandar</td>\n",
       "      <td>24812620</td>\n",
       "      <td>19 Jun 2020</td>\n",
       "      <td>Mature</td>\n",
       "      <td>M/M</td>\n",
       "      <td>Underage</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>3,126</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>825</td>\n",
       "      <td>67</td>\n",
       "      <td>7870</td>\n",
       "      <td>\\nUnderageRegulus Black/Sirius Black Alphard B...</td>\n",
       "      <td>\\nThe morning after the night before. Sirius i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>The Slytherin Scarf</td>\n",
       "      <td>FateRestarting</td>\n",
       "      <td>24812632</td>\n",
       "      <td>19 Jun 2020</td>\n",
       "      <td>Teen And Up Audiences</td>\n",
       "      <td>F/M</td>\n",
       "      <td>No Archive Warnings Apply</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>2,677</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>404</td>\n",
       "      <td>43</td>\n",
       "      <td>3622</td>\n",
       "      <td>\\nNo Archive Warnings ApplyHermione Granger/Dr...</td>\n",
       "      <td>\\nA prefect's round that ends in the usual arg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>Muggle Robbery</td>\n",
       "      <td>Axelle_Sof</td>\n",
       "      <td>24812779</td>\n",
       "      <td>19 Jun 2020</td>\n",
       "      <td>General Audiences</td>\n",
       "      <td>M/M</td>\n",
       "      <td>Choose Not To Use Archive Warnings</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>1,522</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>780</td>\n",
       "      <td>\\nCreator Chose Not To Use Archive WarningsDra...</td>\n",
       "      <td>\\nHarry and Draco go on a date. But not togeth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>Forged in Fire</td>\n",
       "      <td>torino10154</td>\n",
       "      <td>24812839</td>\n",
       "      <td>20 Jun 2020</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>M/M</td>\n",
       "      <td>Rape/Non-Con, Underage</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>857</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>753</td>\n",
       "      <td>50</td>\n",
       "      <td>43545</td>\n",
       "      <td>\\nRape/Non-Con UnderageHarry Potter/Quirinus Q...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>Amortentia</td>\n",
       "      <td>Im_Chamsae</td>\n",
       "      <td>24813910</td>\n",
       "      <td>22 Jun 2020</td>\n",
       "      <td>General Audiences</td>\n",
       "      <td>F/M</td>\n",
       "      <td>No Archive Warnings Apply</td>\n",
       "      <td>Complete Work</td>\n",
       "      <td>English</td>\n",
       "      <td>4,249</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>7</td>\n",
       "      <td>932</td>\n",
       "      <td>\\nNo Archive Warnings ApplyStephanie Brown/Tim...</td>\n",
       "      <td>\\nWhat initially started out as an interest in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3523 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title                   Author        ID  \\\n",
       "0        This Love I Have Inside  stargazing_dreamer_girl  24329029   \n",
       "1                Spelling It Out      Nocturnal_Daydreams  24329044   \n",
       "2                Untamed Journey                 Jetainia  24329050   \n",
       "3                      The Quill            xslytherclawx  24329686   \n",
       "4     Harry Potter Bending Force                   Gman85  24329902   \n",
       "...                          ...                      ...       ...   \n",
       "3518                  Ante Astra                  Evandar  24812620   \n",
       "3519         The Slytherin Scarf           FateRestarting  24812632   \n",
       "3520              Muggle Robbery               Axelle_Sof  24812779   \n",
       "3521              Forged in Fire              torino10154  24812839   \n",
       "3522                  Amortentia               Im_Chamsae  24813910   \n",
       "\n",
       "     Date_updated                 Rating     Pairing  \\\n",
       "0     23 May 2020      General Audiences         F/M   \n",
       "1     23 May 2020              Not Rated  F/M, Other   \n",
       "2     23 May 2020      General Audiences         F/M   \n",
       "3     23 May 2020  Teen And Up Audiences         M/M   \n",
       "4     19 Jan 2022               Explicit         F/M   \n",
       "...           ...                    ...         ...   \n",
       "3518  19 Jun 2020                 Mature         M/M   \n",
       "3519  19 Jun 2020  Teen And Up Audiences         F/M   \n",
       "3520  19 Jun 2020      General Audiences         M/M   \n",
       "3521  20 Jun 2020               Explicit         M/M   \n",
       "3522  22 Jun 2020      General Audiences         F/M   \n",
       "\n",
       "                                 Warning          Complete Language  \\\n",
       "0              No Archive Warnings Apply     Complete Work  English   \n",
       "1     Choose Not To Use Archive Warnings     Complete Work  English   \n",
       "2              No Archive Warnings Apply     Complete Work  English   \n",
       "3              No Archive Warnings Apply     Complete Work  English   \n",
       "4                               Underage  Work in Progress  English   \n",
       "...                                  ...               ...      ...   \n",
       "3518                            Underage     Complete Work  English   \n",
       "3519           No Archive Warnings Apply     Complete Work  English   \n",
       "3520  Choose Not To Use Archive Warnings     Complete Work  English   \n",
       "3521              Rape/Non-Con, Underage     Complete Work  English   \n",
       "3522           No Archive Warnings Apply     Complete Work  English   \n",
       "\n",
       "     Word_count  Num_chapters  Num_comments  Num_kudos  Num_bookmarks  \\\n",
       "0         9,403             1             0         37              1   \n",
       "1        29,407             1            14        213             32   \n",
       "2         1,612             1             0         17              1   \n",
       "3           586             1             4         68              2   \n",
       "4       128,631            17           104        286            133   \n",
       "...         ...           ...           ...        ...            ...   \n",
       "3518      3,126             1            38        825             67   \n",
       "3519      2,677             1            38        404             43   \n",
       "3520      1,522             1             0         43              3   \n",
       "3521        857             1            23        753             50   \n",
       "3522      4,249             1            10         95              7   \n",
       "\n",
       "      Num_hits                                               Tags  \\\n",
       "0          612  \\nNo Archive Warnings ApplyFred Weasley/Origin...   \n",
       "1         5338  \\nCreator Chose Not To Use Archive WarningsHer...   \n",
       "2          240  \\nNo Archive Warnings ApplyHelga Hufflepuff/Sa...   \n",
       "3          664  \\nNo Archive Warnings ApplyDraco Malfoy/Harry ...   \n",
       "4        21617  \\nUnderageOther Relationship Tags to Be Added ...   \n",
       "...        ...                                                ...   \n",
       "3518      7870  \\nUnderageRegulus Black/Sirius Black Alphard B...   \n",
       "3519      3622  \\nNo Archive Warnings ApplyHermione Granger/Dr...   \n",
       "3520       780  \\nCreator Chose Not To Use Archive WarningsDra...   \n",
       "3521     43545  \\nRape/Non-Con UnderageHarry Potter/Quirinus Q...   \n",
       "3522       932  \\nNo Archive Warnings ApplyStephanie Brown/Tim...   \n",
       "\n",
       "                                                Summary  \n",
       "0     \\nClara Comder, student at Hogwarts School of ...  \n",
       "1     \\nTattoos or Birthmarks of your soulmates firs...  \n",
       "2     \\nThe roaming Hogwarts saloon is a place of ha...  \n",
       "3     \\nHarry's always wondered about Draco's golden...  \n",
       "4     \\nHarry was woefully unprepared for the Tri-Wi...  \n",
       "...                                                 ...  \n",
       "3518  \\nThe morning after the night before. Sirius i...  \n",
       "3519  \\nA prefect's round that ends in the usual arg...  \n",
       "3520  \\nHarry and Draco go on a date. But not togeth...  \n",
       "3521                                                     \n",
       "3522  \\nWhat initially started out as an interest in...  \n",
       "\n",
       "[3523 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use read_csv to read the data stored in the CSV files into pandas dataframes\n",
    "AO3 = pd.read_csv(\"storedbasic.csv\")\n",
    "\n",
    "#Display the final dataframe\n",
    "display(AO3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac95180-ffe4-40c0-9bdc-831fd3ae4d7c",
   "metadata": {},
   "source": [
    "### <center>Data Processing</center>\n",
    "\n",
    "We now have a singular data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc9a05-a284-4c45-9849-94e1556f5335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c63034d8-1b40-4b24-a77b-76c690f0ac7b",
   "metadata": {},
   "source": [
    "### <center>Exploratory Data Analysis</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142a931-4f13-4733-acb6-52d025327d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d154d52a-ab4e-4f2e-9e16-a28206a9b440",
   "metadata": {},
   "source": [
    "### <center>Hypothesis Testing</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e29670-cf1c-4808-a033-bb4b347ad92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ac3632-0c14-422f-ad24-a81dc603eb04",
   "metadata": {},
   "source": [
    "### <center>Conclusions</center>\n",
    "\n",
    "We now have a singular data frame "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
