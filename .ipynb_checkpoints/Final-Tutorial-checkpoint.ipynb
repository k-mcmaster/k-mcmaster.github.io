{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50a2c95-610f-480f-9c17-4acfd2669c8d",
   "metadata": {},
   "source": [
    "# <center>An Analysis of Transformative Works Created by Fans of the Harry Potter Series in Reaction to the Author's Public Political Comments</center>\n",
    "\n",
    "## <center>Project completed by Kymberlee McMaster on May 16th, 2022</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb7072-0ffe-4d1a-ad3c-08f3333a4b91",
   "metadata": {},
   "source": [
    "### <center>Introduction</center>\n",
    "\n",
    "Fans are often known for using their talents and dedication to create new media based on the things they enjoy. One of the best examples of this is the writing and reading of fanfiction, the practice in which amateur authors may take aspects from the original content that they enjoyed and transforming them into original works of their own creation. There are various methods that these authors use to share their works with other individuals who also enjoyed the original piece of media but one of the most common is to post the work to a dedicated site for the posting and reading of fanfiction. While there are quite a few options available, we'll be focusing on Archive of Our Own, known colloquially as AO3, for our purposes as AO3's built in tagging and data storage system will allow us to search through the works of fiction using the author's own tags for their work rather than attempting to create tags ourself. \n",
    "\n",
    "However, since AO3 currently has over nine million works, in order to better analyze the data associated with the site and trends of fanfiction authors, we'll be focusing on writings by fans of a specific piece of media: the Harry Potter series written by J.K. Rowling.[[1]](https://archiveofourown.org/works/search?work_search%5Bquery%5D=) Additionally, we'll be specifically be focusing on the fanfiction written around a specific date in time as there are over 300,000 works for that series alone. \n",
    "\n",
    "On June 6th of 2020, author J.K. Rowling took to Twitter to express her displeasure over the use of the phrase \"people who menstruate\" rather than the word women.[[2]](https://www.glamour.com/story/a-complete-breakdown-of-the-jk-rowling-transgender-comments-controversy) This tweet and the subsequent tweets that followed it came under a lot of backlash with trans activists and fans of the Harry Potter series. This was not the first time that author J.K. Rowling had expressed such views and received backlash, but it is one of the most notable, so we will be aanalyzing works of fanfiction posted onto AO3 for the two weeks before the tweet was made and the two weeks following the tweet to view the potential impact that Rowling's postings may have had on the writings of the LGBTQIA+ community members and their allies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b77fe-964f-44f7-8be8-e39bc29909aa",
   "metadata": {},
   "source": [
    "### <center>Data Collection</center>\n",
    "\n",
    "As AO3 does not have a built in API, we will need to build our own method of scraping the data found on the site. In order to collect the data and avoid unneccesary scraping we'll be using AO3's built in search function to pre-search for works that were created between our dates of interest: May 23rd,2020 and June 19th, 2020. We do this by accessing the Works Search page located [here](https://archiveofourown.org/works/search), and entering our parameters into the Any Search field: created_at:[\"2020-05-23\" TO \"2020-06-19\"]. As well as selecting the English language option. This will generate the link that we can use in the data scraper that will gather the information about the works for us, located [here](https://archiveofourown.org/works/search?commit=Search&page=1&work_search[bookmarks_count]=&work_search[character_names]=&work_search[comments_count]=&work_search[complete]=&work_search[creators]=&work_search[crossover]=&work_search[fandom_names]=Harry+Potter+-+J.+K.+Rowling&work_search[freeform_names]=&work_search[hits]=&work_search[kudos_count]=&work_search[language_id]=en&work_search[query]=created_at%3A[%222020-05-23%22+TO+%222020-06-19%22]&work_search[rating_ids]=&work_search[relationship_names]=&work_search[revised_at]=&work_search[single_chapter]=0&work_search[sort_column]=created_at&work_search[sort_direction]=asc&work_search[title]=&work_search[word_count]=). \n",
    "\n",
    "By looking at our search results, we will see that we will be scraping the information about 3,523 works that were created and area available publicly without an account in the month time period we've identified. Below, we will first import the libararies necessary for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ef4c48-3b62-48a0-b4bc-0de6f8efaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries necessary to complete this project \n",
    "import requests\n",
    "import math\n",
    "from bs4 import BeautifulSoup \n",
    "import csv\n",
    "import re \n",
    "import random\n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import datetime \n",
    "\n",
    "\n",
    "import json \n",
    "import os.path \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3800a9-3871-4a4f-be45-ae364192eebf",
   "metadata": {},
   "source": [
    "Next, we need to inspect the page to see how the data on each page is stored. By using the developers tools, we can see that the results of the search are displayed in a class identified as a â€œworks index group\" and each work is a list item below that with the role \"article\". We know that there are 3,523 works to be consumed, and there are 20 works displayed on each search page so we'll need to request the informtion from 177 pages. We split the URL into parts before and after the page number is stored to that we can complete our requests through an iterative process which automatically updates the page number used to request data. Then we initialize a couple new files to store the content from the scraping as we'll be completing it in page portions and that would make it difficult to store as a pandas dataframe right off the bat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b645645a-3f04-4eb3-9936-42e3ac15bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the URL into parts and store the current page number with those parts \n",
    "urlpt1 = \"https://archiveofourown.org/works/search?commit=Search&page=\"\n",
    "currpagenum = 1\n",
    "urlpt2 = \"&work_search[bookmarks_count]=&work_search[character_names]=&work_search[comments_count]=&work_search[complete]=&work_search[creators]=&work_search[crossover]=&work_search[fandom_names]=Harry+Potter+-+J.+K.+Rowling&work_search[freeform_names]=&work_search[hits]=&work_search[kudos_count]=&work_search[language_id]=en&work_search[query]=created_at%3A[%222020-05-23%22+TO+%222020-06-19%22]&work_search[rating_ids]=&work_search[relationship_names]=&work_search[revised_at]=&work_search[single_chapter]=0&work_search[sort_column]=created_at&work_search[sort_direction]=asc&work_search[title]=&work_search[word_count]=\"\n",
    "\n",
    "#Identified the number of works and pages that the scraper will need to iterate through\n",
    "works = 3523\n",
    "pages = math.ceil(works/20)\n",
    "\n",
    "#Iniate a new file to store the basic content from the scraping \n",
    "header = ['Title', 'Author', 'ID', 'Date_updated', 'Rating', 'Pairing', 'Warning', 'Complete', 'Language', 'Word_count', 'Num_chapters', 'Num_comments', 'Num_kudos', 'Num_bookmarks', 'Num_hits', 'Tags', 'Summary']\n",
    "with open('storedbasic.csv','w', encoding='utf8') as storedbasic:\n",
    "    writer = csv.writer(storedbasic)\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b8e33-ef19-4dd5-8b3a-b256603d9318",
   "metadata": {},
   "source": [
    "Now that we've completed some of our basic work, we can begin to design some of the functions we'll need to call to scrape the data out of the page. Specifically, we'll need one function to read in the content off the page. One piece of information that we could scrape but are choosing not to for the purpose of this project is the actual Comments on the published works. While this could have some interested information for us to take a look at, we are already dealing with an extremely large amount of data and the Comments section of each work does not have any bearing on what our true aim is with this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d49c95-6902-4238-bdf4-ddc7f784cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to gather all data \n",
    "def basicdata(mysoup): \n",
    "    #Initialize a set of variables to store all titles and info for page to add to the CSV all at once \n",
    "    titles = []\n",
    "    authors = []\n",
    "    ids = []\n",
    "    date_updated = []\n",
    "    ratings = []\n",
    "    pairings = []\n",
    "    warnings = []\n",
    "    complete = []\n",
    "    languages = []\n",
    "    word_count = []\n",
    "    chapters = []\n",
    "    comments = []\n",
    "    kudos = []\n",
    "    bookmarks = []\n",
    "    hits = []\n",
    "    tags = []\n",
    "    summary = []\n",
    "    \n",
    "    for article in mysoup.find_all('li', {'role':'article'}):\n",
    "        titles.append(article.find('h4', {'class':'heading'}).find('a').text)\n",
    "        try:\n",
    "            authors.append(article.find('a', {'rel':'author'}).text)\n",
    "        except:\n",
    "            authors.append('Anonymous')\n",
    "        ids.append(article.find('h4', {'class':'heading'}).find('a').get('href')[7:])\n",
    "        date_updated.append(article.find('p', {'class':'datetime'}).text)\n",
    "        ratings.append(article.find('span', {'class':re.compile(r'rating\\-.*rating')}).text)\n",
    "        pairings.append(article.find('span', {'class':re.compile(r'category\\-.*category')}).text)\n",
    "        warnings.append(article.find('span', {'class':re.compile(r'warning\\-.*warnings')}).text)\n",
    "        complete.append(article.find('span', {'class':re.compile(r'complete\\-.*iswip')}).text)\n",
    "        languages.append(article.find('dd', {'class':'language'}).text)\n",
    "        tags.append(article.find('ul', {'class':'tags commas'}).text)\n",
    "        count = article.find('dd', {'class':'words'}).text\n",
    "        if len(count) > 0:\n",
    "            word_count.append(count)\n",
    "        else:\n",
    "            word_count.append('0')\n",
    "        chapters.append(article.find('dd', {'class':'chapters'}).text.split('/')[0])\n",
    "        try:\n",
    "            comments.append(article.find('dd', {'class':'comments'}).text)\n",
    "        except:\n",
    "            comments.append('0')\n",
    "        try:\n",
    "            kudos.append(article.find('dd', {'class':'kudos'}).text)\n",
    "        except:\n",
    "            kudos.append('0')\n",
    "        try:\n",
    "            bookmarks.append(article.find('dd', {'class':'bookmarks'}).text)\n",
    "        except:\n",
    "            bookmarks.append('0')\n",
    "        try:\n",
    "            hits.append(article.find('dd', {'class':'hits'}).text)\n",
    "        except:\n",
    "            hits.append('0')\n",
    "        #try: \n",
    "            #tags.append(article.find('span', {'class':re.compile(r'freeforms\\-.*freeforms')}).text)\n",
    "        #except: \n",
    "            #tags.append(' ')\n",
    "        try:\n",
    "            summary.append(article.find('blockquote', {'class':'userstuff summary'}).text)\n",
    "        except: \n",
    "            summary.append(' ')\n",
    "            \n",
    "            \n",
    "    df = pd.DataFrame(list(zip(titles, authors, ids, date_updated, ratings, pairings,\\\n",
    "                              warnings, complete, languages, word_count, chapters,\\\n",
    "                               comments, kudos, bookmarks, hits, tags, summary)))\n",
    "    \n",
    "    with open('storedbasic.csv','a', encoding='utf8') as storedbasic:\n",
    "        df.to_csv(storedbasic, header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870da39-bc14-4c6c-991d-8f84693fe196",
   "metadata": {},
   "source": [
    "With our helper function for our basic data, we can now  iterate through the pages of the searched works and gather the basic data into the CSV files previously created. Due to AO3's built in site protections, We will scrape by increments of 100 pages and pause between each code block execution in order to ensure that we can gather all of the data we are trying to request from the site.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1a862a-4514-4178-8b0d-a39f8b45c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing has finished, first 100 pages of basic data has been consumed, waiting 5 min before the remaining data consumption\n",
      "Parsing has finished, the remainder of basic data has been consumed\n"
     ]
    }
   ],
   "source": [
    "#Reset page number in case anything has gotten messed up with the block\n",
    "currpagenum = 1\n",
    "\n",
    "#Set the page by using the page number, and the URL parts\n",
    "page = requests.get(urlpt1 + str(currpagenum) + urlpt2)\n",
    "\n",
    "#Use BeautifulSoup to parse the data as html\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "#This for loop will iterate through the pages and add the basic data to the basic data table \n",
    "for i in range(100): \n",
    "    \n",
    "    url = urlpt1 + str(currpagenum) + urlpt2 \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    basicdata(soup) \n",
    "    \n",
    "    currpagenum += 1\n",
    "    \n",
    "print(\"Parsing has finished, first 100 pages of basic data has been consumed, waiting 10 min before the remaining data consumption\")\n",
    "\n",
    "time.sleep(600) \n",
    "\n",
    "#This for loop will iterate through the pages and add the basic data to the basic data table \n",
    "for i in range(100,200): \n",
    "    \n",
    "    url = urlpt1 + str(currpagenum) + urlpt2 \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    basicdata(soup) \n",
    "    \n",
    "    currpagenum += 1\n",
    "    \n",
    "print(\"Parsing has finished, the remainder of basic data has been consumed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e8679-09bf-41ab-bd4c-9803052e62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This for loop will iterate through the pages and add the basic data to the basic data table \n",
    "for i in range(100,200): \n",
    "    \n",
    "    url = urlpt1 + str(currpagenum) + urlpt2 \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    basicdata(soup) \n",
    "    \n",
    "    currpagenum += 1\n",
    "    \n",
    "print(\"Parsing has finished, the remainder of basic data has been consumed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ead2c-5f42-4736-b382-85c17e531ed3",
   "metadata": {},
   "source": [
    "The above print statements are simply to show that the parser has completed running and is moving on to the next code snippet where we will store the information we've collected in a pandas dataframe and verify that our parsing was successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872344c6-4c1e-4ddb-80b5-a399d07de9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use read_csv to read the data stored in the CSV files into pandas dataframes\n",
    "AO3 = pd.read_csv(\"storedbasic.csv\")\n",
    "\n",
    "#Display the final dataframe\n",
    "display(AO3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac95180-ffe4-40c0-9bdc-831fd3ae4d7c",
   "metadata": {},
   "source": [
    "### <center>Data Processing</center>\n",
    "\n",
    "We now have a singular data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc9a05-a284-4c45-9849-94e1556f5335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c63034d8-1b40-4b24-a77b-76c690f0ac7b",
   "metadata": {},
   "source": [
    "### <center>Exploratory Data Analysis</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142a931-4f13-4733-acb6-52d025327d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d154d52a-ab4e-4f2e-9e16-a28206a9b440",
   "metadata": {},
   "source": [
    "### <center>Hypothesis Testing</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e29670-cf1c-4808-a033-bb4b347ad92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ac3632-0c14-422f-ad24-a81dc603eb04",
   "metadata": {},
   "source": [
    "### <center>Conclusions</center>\n",
    "\n",
    "We now have a singular data frame "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
